---
layout: post
title:  " 머신러닝의 분류와 용어 정리"
subtitle:  " 머신러닝의 분류와 각 학습에 대해 알아보자 "
categories: back-end
tags: server
comments: true

---



#### 머신러닝의 분류

머신러닝은 학습하려는 문제의 유형에 따라 크게 다음과 같은 세 가지로 분류할 수 있습니다.

 1. 지도 학습(Supervised Learning)

 2. 비지도 학습(Unsupervised Learning)

 3. 강화 학습(Reinforcement Learning)

   4.준지도학습(Semi-Supervised Learning) 





#### 지도 학습(Supervised Learning)

지도 학습(Supervised Learning)이란 간단히 말해 선생님이 문제를 내고 그 다음 바로 정답까지 같이 알려주는 방식의 학습 방법입니다. 즉, 여러 문제와 답을 같이 학습함으로써 미지의 문제에 대한 올바른 답을 예측하고자 하는 방법입니다. 따라서 지도 학습을 위한 데이터에는 문제와 함께 그 정답까지 함께 알고 있는 데이터가 선택됩니다.



#### 지도 학습의 종류

1. **Binary Classification**

   : 공부시간에 따른 시험 결과 Pass / Fail 을 예측하는 방법

2. **Multi-label Classification**

   : 공부시간에 따른 학점을 A / B / C / D / F 등으로 예측하는 방법

3. **Regression**

   : 공부시간에 따른 시험 점수(0점~100점과 같은 범위)를 예측하는 방법



#### 비지도 학습(Unsupervised Learning)

문제와 함께 정답(레이블)까지 알려주는 지도 학습과는 달리 비지도 학습(Unsupervised Learning)은 문제는 알려주되 정답까지는 알려주지 않는 학습 방식입니다. 즉, 여러 문제를 학습함으로써 해당 데이터의 패턴, 특성 및 구조를 스스로 파악하여, 이를 통해 새로운 데이터에서 일정한 규칙성을 찾는 방법입니다.

비지도 학습은 구체적인 결과에 대한 사전 지식은 없지만 해당 결과 데이터를 통해 유의미한 지식을 얻고자 할 때 사용되며, 사람도 제대로 알 수 없는 본질적인 문제나 데이터에 숨겨진 특징이나 구조 등을 연구할 때 많이 활용됩니다. 머신러닝에서 비지도 학습을 위한 모델로는 군집화(clustering)가 대표적입니다.



#### 군집화(clustering)



비지도 학습에 사용되는 데이터에는 레이블(label)이 명시되어 있지 않기 때문에 지도 학습과는 또 다른 방식으로 학습을 수행해야 합니다.

이때 가장 많이 사용되는 방법이 바로 입력된 데이터가 어떤 형태로 서로 그룹을 형성하는지를 파악하는 것입니다.

비지도 학습에서 가장 대표적으로 사용되는 이 모델은 군집화(clustering) 또는 클러스터링이라고 불립니다.

군집화는 레이블이 없는 학습 데이터들의 특징(feature)을 분석하여 서로 동일하거나 유사한 특징을 가진 데이터끼리 그룹화 함으로써 레이블이 없는 학습 데이터를 군집(cluster, 그룹)으로 분류합니다. 그리고 새로운 데이터가 입력되면 지도 학습의 분류 모델처럼 학습한 군집을 가지고 해당 데이터가 어느 군집에 속하는지를 분석하는 것입니다.

 ![clustering](https://camo.githubusercontent.com/8cdfc2fb6d5c41b54f20c0444b6016f79576971f/687474703a2f2f7463707363686f6f6c2e636f6d2f6c656374757265732f696d675f646565706c6561726e696e675f30365f636c7573746572696e672e706e67) 

#### 군집(cluster)의 타당성 평가



비지도 학습에 사용되는 데이터에는 레이블(label)이 없으므로, 지도 학습처럼 단순정확도(accuracy)를 지표로 그 정확도를 평가할 수는 없습니다.

 ![eval](https://camo.githubusercontent.com/142bb1b20eed0079355782f0b4cb1667c10f2dcb/687474703a2f2f7463707363686f6f6c2e636f6d2f6c656374757265732f696d675f646565706c6561726e696e675f30365f6576616c2e706e67) 

군집을 만든 결과가 얼마만큼 타당한지는 군집간의 거리, 군집의 지름, 군집의 분산도 등을 종합적으로 고려하여 평가할 수 있습니다.

따라서 일반적으로 군집 간 분산(inter-cluster variance)이 최대가 되고 군집 내 분산(inner-cluster variance)이 최소가 될 때 최적의 군집 모양과 개수라고 판단하고 있습니다.



#### 강화 학습(Reinforcement Learning)

지도 학습과 비지도 학습이 학습 데이터가 주어진 상태에서 환경에 변화가 없는 정적인 환경에서 학습을 진행했다면, 강화 학습은 어떤 환경 안에서 정의된 주체(agent)가 현재의 상태(state)를 관찰하여 선택할 수 있는 행동(action)들 중에서 가장 최대의 보상(reward)을 가져다주는지 행동이 무엇인지를 학습하는 것입니다.

강화 학습은 주체(agent)가 환경으로부터 보상을 받음으로써 학습하기 때문에 지도 학습과 유사해 보이지만, 사람으로부터 학습을 받는 것이 아니라 변화되는 환경으로부터 보상을 받아 학습한다는 점에서 차이를 보입니다.





#### 강화 학습의 동작 순서



강화 학습은 일반적으로 다음과 같은 순서대로 학습을 진행하게 됩니다.

 1. 정의된 주체(agent)가 주어진 환경(environment)의 현재 상태(state)를 관찰(observation)하여, 이를 기반으로 행동(action)을 취합니다.

 2. 이때 환경의 상태가 변화하면서 정의된 주체는 보상(reward)을 받게 됩니다.

 3. 이 보상을 기반으로 정의된 주체는 더 많은 보상을 얻을 수 있는 방향(best action)으로 행동을 학습하게 됩니다.

강화 학습에서의 ‘관찰–행동–보상’에 이르는 일련의 과정을 경험(experience)이라고 부를 수 있습니다.



#### 이용(exploitation)과 탐험(exploration) 사이의 균형



경험을 통해 학습하는 강화 학습에서 최단 시간에 주어진 환경의 모든 상태를 관찰하고, 이를 기반으로 보상을 최대화할 수 있는 행동을 수행하기 위해서는 이용(exploitation)과 탐험(exploration) 사이의 균형을 적절히 맞춰야 합니다.

이용(exploitation)이란 현재까지의 경험 중 현 상태에서 가장 최대의 보상을 얻을 수 있는 행동을 수행하는 것을 의미하고, 이러한 다양한 경험을 쌓기 위해서는 새로운 시도가 필요한데 이러한 새로운 시도를 탐험(exploration)이라고 합니다.

탐험을 통해 얻게 되는 경험이 언제나 최상의 결과일 수는 없기에 이 부분에서 낭비가 발생하게 됩니다. 즉, 풍부한 경험이 있어야만 더 좋은 선택을 할 수 있게 되지만, 경험을 풍부하게 만들기 위해서는 새로운 시도를 해야 하고 이러한 새로운 시도는 언제나 위험 부담을 가지게 됩니다.



예를 들어, 빵집에 가서 지금까지 자신이 먹어본 빵 중 가장 맛있는 빵을 고르는 것이 이용(exploitation)이 되며, 한 번도 먹어보지 못한 다른 빵을 고르는 것이 탐험(exploration)이 됩니다. 만약 새로 고른 빵이 가장 맛있다고 느껴지면 다음 번 선택에서 이용될 수 있으나, 만약 맛이 없었다면 한 번의 기회를 낭비하게 된 것입니다.

따라서 이용과 탐험 사이의 적절한 균형을 맞추는 것이 강화 학습의 핵심이 되는 것입니다.





#### 마르코프 결정 프로세스(Markov Decision Process, MDP)



강화 학습에서 보상을 최대화할 수 있는 방향으로 행동을 취할 수 있도록 이용과 탐험 사이의 적절한 균형을 맞추는데 사용되는 의사결정 프로세스가 바로 마르코프 결정 프로세스(Markov Decision Process, MDP)입니다.

MDP에서 행위의 주체(agent)는 어떤 상태(state)를 만나면 행동(action)을 취하게 되며, 각 상태에 맞게 취할 수 있는 행동을 연결해 주는 함수를 정책(policy)이라고 합니다. 따라서 MDP는 행동을 중심으로 가치 평가가 이루어지며, MDP의 가장 큰 목적은 가장 좋은 의사결정 정책(policy) 즉 행동에 따른 가치(value)의 합이 가장 큰 의사결정 정책을 찾아내는 것입니다.



### 준지도 학습(Semi-Supervised Learning) 

​    : 학습 데이터가 어느 정도만의 Label을 가지고 있도록 학습하는 방법